{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 langchain langchain_community huggingface_hub transformers sentence-transformers google-colab"
      ],
      "metadata": {
        "id": "rpg1Xf0AK8WZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "id": "X5CBCaruLfYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "hf_api_key=userdata.get('HUGGINGFACEHUB_TOKEN')"
      ],
      "metadata": {
        "id": "t33MLBkQMEnR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade tensorflow"
      ],
      "metadata": {
        "id": "pUbe17vjnJBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.12.0\n",
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "GbIPjlUXrJm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "W8r8BPLYKdGB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import concurrent.futures\n",
        "from dotenv import load_dotenv\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "from google.colab import files\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Hugging Face API Key\n",
        "# hf_api_key = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
        "\n",
        "# Function to extract text from PDFs\n",
        "def get_pdf_text(pdf_files):\n",
        "    text = \"\"\n",
        "\n",
        "    def extract_text(pdf):\n",
        "        pdf_reader = PdfReader(pdf)\n",
        "        pdf_text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                pdf_text += page_text + \"\\n\"\n",
        "        return pdf_text\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "        futures = [executor.submit(extract_text, pdf) for pdf in pdf_files]\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            text += future.result()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Function to split text into chunks\n",
        "def get_text_chunks(text):\n",
        "    text_splitter = CharacterTextSplitter(\n",
        "        separator=\"\\n\",\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200,\n",
        "        length_function=len\n",
        "    )\n",
        "    return text_splitter.split_text(text)\n",
        "\n",
        "# Function to create FAISS vector store\n",
        "def get_vectorstore(text_chunks):\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n",
        "    return vectorstore\n",
        "\n",
        "# Function to create a conversational chain\n",
        "def get_conversation_chain(vectorstore):\n",
        "    llm = HuggingFaceHub(\n",
        "        repo_id=\"yeontaek/airoboros-2.1-llama-2-13B-QLoRa\",\n",
        "        model_kwargs={\"temperature\": 0.5, \"max_length\": 512},\n",
        "        huggingfacehub_api_token=hf_api_key\n",
        "    )\n",
        "\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key='chat_history', return_messages=True\n",
        "    )\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        memory=memory\n",
        "    )\n",
        "    return conversation_chain\n",
        "\n",
        "# Upload PDFs in Colab\n",
        "print(\"Upload your PDFs\")\n",
        "uploaded_files = files.upload()\n",
        "\n",
        "pdf_docs = list(uploaded_files.keys())\n",
        "print(\"Processing PDFs...\")\n",
        "\n",
        "# Extract text from PDFs\n",
        "raw_text = get_pdf_text(pdf_docs)\n",
        "\n",
        "# Get text chunks\n",
        "text_chunks = get_text_chunks(raw_text)\n",
        "\n",
        "# Create FAISS vector store\n",
        "vectorstore = get_vectorstore(text_chunks)\n",
        "\n",
        "# Initialize conversation chain\n",
        "conversation = get_conversation_chain(vectorstore)\n",
        "print(\"PDFs processed successfully! You can now ask questions.\")\n",
        "\n",
        "# Chat loop\n",
        "while True:\n",
        "    user_question = input(\"Ask a question (or type 'exit' to quit): \")\n",
        "    if user_question.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    response = conversation({'question': user_question})\n",
        "    chat_history = response['chat_history']\n",
        "\n",
        "    for i, message in enumerate(chat_history):\n",
        "        if i % 2 == 0:\n",
        "            print(f\"User: {message.content}\")\n",
        "        else:\n",
        "            print(f\"Bot: {message.content}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XwmNIsYhLIP5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}